{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mechanize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-11dc172d1a2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmechanize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlogging\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasicConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mechanize'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr  2 21:17:38 2014\n",
    "\n",
    "google_news.py scrapes news headlines and the name of their outlets from the\n",
    "Google News homepage on a set schedule. The readability of the headlines is\n",
    "assessed with the Flesch-Kincaid Grade Level test.\n",
    "\n",
    "After all the scheduled jobs are run, the data are cleaned: badly-formed text,\n",
    "non-sensical results, and duplicate records are reformatted or removed. The\n",
    "cleaned data are analyzed at the level of news outlets.\n",
    "\n",
    "Finally, google_news.R is called to create a visualization of the results.\n",
    "\n",
    "@author: Juan Manuel Contreras (juan.manuel.contreras.87@gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "# Import modules\n",
    "from time import sleep\n",
    "from numpy import mean\n",
    "from time import strftime\n",
    "from os.path import isfile\n",
    "from scipy.stats import sem\n",
    "from subprocess import call\n",
    "from mechanize import Browser\n",
    "from logging import basicConfig\n",
    "from lxml.html import fromstring\n",
    "from pandas import DataFrame, isnull\n",
    "from apscheduler.scheduler import Scheduler\n",
    "from re import finditer, search, IGNORECASE\n",
    "from readability.readability import Readability\n",
    "    \n",
    "def assess_readability(text):\n",
    "\n",
    "    '''Assess the readability of text with the Flesch-Kincaid Grade Level test,\n",
    "    as implemented in Python here: https://github.com/mmautner/readability'''\n",
    "    \n",
    "    # Assess grade level    \n",
    "    return Readability(text).FleschKincaidGradeLevel()\n",
    "\n",
    "def scrape(file_name):\n",
    "    \n",
    "    '''Scrape headlines and news outlet names from the Google News homepage,\n",
    "       assessing their readability and writing or appendin to a CSV file'''\n",
    "    \n",
    "    # Get the current date and time\n",
    "    date = strftime('%m/%d/%y')\n",
    "    time = strftime('%H:%M:%S')\n",
    "    \n",
    "    # Construct browser object\n",
    "    browser = Browser()\n",
    "    \n",
    "    # Do not observe rules from robots.txt\n",
    "    browser.set_handle_robots(False)\n",
    "    \n",
    "    # Create HTML document\n",
    "    html = fromstring(browser.open('https://news.google.com/').read())\n",
    "    \n",
    "    # Declare outlets and titles\n",
    "    outlets = html.xpath('.//*[@class=\"esc-lead-article-outlet-wrapper\"]')\n",
    "    titles = html.xpath('.//*[@class=\"esc-lead-article-title\"]')\n",
    "    \n",
    "    # Number of items\n",
    "    n_items = len(titles)\n",
    "    \n",
    "    # Initialize empty Pandas data frame\n",
    "    empty_list = [None] * n_items\n",
    "    df = DataFrame({'outlet': empty_list,\n",
    "                    'title': empty_list,\n",
    "                    'flesch': empty_list,\n",
    "                    'date_time': [date + ' ' + time] * n_items})\n",
    "    \n",
    "    # Iterate through outlets and titles\n",
    "    for i in xrange(n_items):\n",
    "        \n",
    "        # Declare raw outlet name\n",
    "        raw_outlet = outlets[i].text_content()\n",
    "        \n",
    "        # Find the last meaningful character in the raw_outlet string\n",
    "        try:\n",
    "            last_char = raw_outlet.index('-') - 1\n",
    "        except ValueError as e:\n",
    "            if e.message == 'substring not found':\n",
    "                last_char = search('\\d', raw_outlet).start()\n",
    "        \n",
    "        # Slice the raw outlet name into something useable                \n",
    "        this_outlet = raw_outlet[:last_char]\n",
    "        this_title = titles[i].text_content().encode('utf8')\n",
    "\n",
    "        # Input results into data frame\n",
    "        df.outlet[i] = this_outlet\n",
    "        df.title[i] = this_title\n",
    "        df.flesch[i] = assess_readability(this_title)\n",
    "    \n",
    "    # If a file exists, then append results to it; otherwise, create a file\n",
    "    if isfile(file_name):\n",
    "        df.to_csv(file_name, header=False, index=False, mode='a')\n",
    "        report_str = 'Appended '\n",
    "    else:\n",
    "        df.to_csv(file_name, index=False)\n",
    "        report_str = 'Created %s with ' % (file_name)\n",
    "        \n",
    "    # Report progress\n",
    "    print( '%s%s headlines on %s at %s' % (report_str, n_items, date, time))\n",
    "\n",
    "def schedule(file_name, n_jobs, frequency):\n",
    "\n",
    "    '''Schedule the scraper to execute every hour and shut it down after a\n",
    "       certain number of jos have been run'''\n",
    "\n",
    "    # Create a default logger\n",
    "    basicConfig()\n",
    "\n",
    "    # Run the first job\n",
    "    scrape(file_name)\n",
    "\n",
    "    # Instantiate the scheduler\n",
    "    sched = Scheduler()\n",
    "    \n",
    "    # Start it\n",
    "    sched.start()\n",
    "\n",
    "    # Schedule the function\n",
    "    sched.add_interval_job(scrape, args=[file_name], minutes=frequency,\n",
    "                           misfire_grace_time=60)\n",
    "    \n",
    "    # Wait to run n_jobs (assuming 1 job per hour, which is 3600 seconds)\n",
    "    sleep(n_jobs * 3600)\n",
    "    \n",
    "    # Shutdown the scheduler\n",
    "    sched.shutdown()\n",
    "    \n",
    "def clean(file_name):\n",
    "    \n",
    "    '''Clean the data collected, including removing duplicate headlines, and\n",
    "       save the results to a different CSV file'''\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = DataFrame.from_csv(path=file_name, index_col=False)\n",
    "    \n",
    "    # Declare patterns found in dirty news outlet and headline records,\n",
    "    # including improper encoding\n",
    "    s_pattern = '\\d+ minute|\\d+ hour| \\(\\w+tion\\)| \\(blog\\)'\n",
    "    t_pattern = '\\[video\\]| \\(\\+video\\)'\n",
    "    e_pattern = '\\x89\\xdb\\xd2 '\n",
    "    \n",
    "    # Initialize empty list of records to be removed\n",
    "    remove = []\n",
    "    \n",
    "    # Iterate through the records\n",
    "    for i in xrange(df.shape[0]):\n",
    "        # Define the news outlet and headline of the iteration\n",
    "        s = df.outlet[i]\n",
    "        t = df.title[i].lower()\n",
    "        # Tag records if the outlet is empty or non-sensical\n",
    "        if isnull(s) or s == '(multiple names)':\n",
    "            remove.append(i)\n",
    "            continue\n",
    "        # Clean the outlet, if necessary\n",
    "        if search(s_pattern, s):\n",
    "            df.outlet[i] = s[:[m.start() for m in finditer(s_pattern, s)][0]]\n",
    "        # Clean title with encoding error\n",
    "        if search(e_pattern, t):\n",
    "            t = t.replace(e_pattern, '')\n",
    "        # Remove extra letters from Christian Science Monitor name\n",
    "        if s.endswith('MonitorApr'):\n",
    "            df.outlet[i] = s[:-3]\n",
    "        # In titles with plus signs instead of whitespaces, perform replacement\n",
    "        if t.count(' ') == 0 and t.count('+') > 0:\n",
    "            df.title[i] = t.replace('+', ' ')\n",
    "            df.flesch[i] = assess_readability(df.title[i])\n",
    "        # Remove video references from headlines\n",
    "        if search(t_pattern, t, IGNORECASE):                \n",
    "            span = [m.span() for m in finditer(t_pattern, t, IGNORECASE)][0]\n",
    "            df.title[i] = t[:span[0]] + t[span[1]:]\n",
    "            df.flesch[i] = assess_readability(df.title[i])\n",
    "        # Remove information about the time when the article was posted\n",
    "        if search(' \\- Hours', t):\n",
    "            df.title[i] = t[:[m.start() for m in finditer(' \\- Hours', t)][0]]\n",
    "            df.flesch[i] = assess_readability(df.title[i])\n",
    "        # Tag records with metadata instead of headlines\n",
    "        if t.startswith('Written by '):\n",
    "            remove.append(i)\n",
    "    \n",
    "    # Remove unsalvageable records\n",
    "    df = df.drop(df.index[remove])\n",
    "    \n",
    "    # Drop duplicate records\n",
    "    df['title_lower'] = [t.lower() for t in df.title]\n",
    "    df.drop_duplicates(cols=['title_lower', 'outlet'], inplace=True)\n",
    "    df.drop(labels='title_lower', axis=1, inplace=True)\n",
    "    \n",
    "    # Save clean data to new file\n",
    "    df.to_csv(path_or_buf=file_name.replace('.', '_clean.'), index=False)\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "def grades2schools(df):\n",
    "    \n",
    "    '''Determine the school (elementary, middle, high, or college+) a reader\n",
    "       needs to attend to parse the headline'''\n",
    "    \n",
    "    # Transform each grade to its school equivalent\n",
    "    df['elem'] = (df.flesch >= 1) & (df.flesch < 6)\n",
    "    df['middle'] = (df.flesch >= 6) & (df.flesch < 9)\n",
    "    df['high'] = (df.flesch >= 9) & (df.flesch < 13)\n",
    "    df['college'] = df.flesch >= 13\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "  \n",
    "def print_stats(data, stats):\n",
    "    \n",
    "    '''Print some relevant statistics'''    \n",
    "    \n",
    "    def print_percent(x):\n",
    "        return round(x.sum() / x.shape[0] * 100, 2)\n",
    "        \n",
    "#     print '\\nFLESCH STATISTICS'\n",
    "#     print 'Mean = %s' % (round(data.flesch.mean(), 2))\n",
    "#     print 'SD   = %s' % (round(data.flesch.std(), 2))\n",
    "#     print 'Min  = %s' % (data.flesch.min())\n",
    "#     print 'Max  = %s' % (data.flesch.max())\n",
    "        \n",
    "#     print '\\nSCHOOL PERCENTAGES'\n",
    "#     print 'Elementary  = %s%%' % (print_percent(data.elem))\n",
    "#     print 'Middle      = %s%%' % (print_percent(data.middle))\n",
    "#     print 'High        = %s%%' % (print_percent(data.high))\n",
    "#     print 'SomeCollege = %s%%' % (print_percent(data.college))\n",
    "    \n",
    "#     print '\\nHEADLINES BY OUTLET'\n",
    "#     print 'Mean = %s' % (round(stats.n_headlines.mean()))\n",
    "#     print 'SD   = %s' % (round(stats.n_headlines.std()))\n",
    "#     print 'Min  = %s' % (stats.n_headlines.min())\n",
    "#     print 'Max  = %s' % (stats.n_headlines.max())\n",
    "  \n",
    "def main():\n",
    "    \n",
    "    # Name the CSV file\n",
    "    file_name = 'google_news.csv'\n",
    "    \n",
    "    # Schedule and run the scraper\n",
    "    schedule(file_name=file_name, n_jobs=10, frequency=30)\n",
    "\n",
    "    # Clean data and save them to a different file\n",
    "    df = clean(file_name)\n",
    "        \n",
    "    # Transform grades to schools (elementary, middle, high, and some college)\n",
    "    df = grades2schools(df)\n",
    "    \n",
    "    # Group items by outlet\n",
    "    grouped = df.groupby(by='outlet', as_index=False)\n",
    "    \n",
    "    # Compute aggregate Flesch statistcs\n",
    "    flesch_dict = {'mean': mean, 'sem': sem, 'n_headlines': len}\n",
    "    flesch_stats = grouped['flesch'].agg(flesch_dict)\n",
    "    \n",
    "    # Restrict news outlets to those with at least 100 headlines\n",
    "    flesch_stats = flesch_stats[flesch_stats.n_headlines >= 100]\n",
    "    \n",
    "    # Apply the same restriction to the non-aggregated data\n",
    "    df = df[df.outlet.isin(list(set(flesch_stats.outlet)))]\n",
    "    \n",
    "    # Print statistics\n",
    "    print_stats(data=df, stats=flesch_stats)\n",
    "    \n",
    "    # Save aggregated data\n",
    "    flesch_stats.to_csv(file_name.replace('.', '_aggregate.'))\n",
    "    \n",
    "    # Plot results\n",
    "    Rscript = '/Library/Frameworks/R.framework/Versions/3.0/Reoutlets/Rscript'\n",
    "    call([Rscript, '/Users/jmcontreras/Github/google-news/google_news.R'])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
